{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3bd06-f806-4ebc-9cd5-3cd9c6c8c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import sympy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, r2_score\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.initializers import Zeros, RandomNormal\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "import qkeras\n",
    "from qkeras import *\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d6194a-1cad-410c-b27d-d1f43f203821",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b20a39b-733c-480e-9120-1835b52be986",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = sorted(glob.glob('recon3D/recon3D_d*.parquet'))\n",
    "labels_files = sorted(glob.glob('labels/labels_d*.parquet'))\n",
    "\n",
    "chunk_size = 10\n",
    "data_chunks = []\n",
    "labels_chunks = []\n",
    "n_files = 0\n",
    "\n",
    "for i in range(0, len(data_files), chunk_size):\n",
    "    data_chunk_files = data_files[i:i+chunk_size]\n",
    "    labels_chunk_files = labels_files[i:i+chunk_size]\n",
    "    \n",
    "    data_chunk_list = []\n",
    "    labels_chunk_list = []\n",
    "    \n",
    "    for rf, lf in zip(data_chunk_files, labels_chunk_files):\n",
    "        d = pd.read_parquet(rf)\n",
    "        l = pd.read_parquet(lf)\n",
    "        \n",
    "        data_chunk_list.append(d)\n",
    "        labels_chunk_list.append(l[['y-local', 'pt']])\n",
    "        \n",
    "        n_files += 1\n",
    "        print(f\"Processed {n_files} files\", end='\\r')\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    chunk_data_df = pd.concat(data_chunk_list, ignore_index=True)\n",
    "    chunk_labels_df = pd.concat(labels_chunk_list, ignore_index=True)\n",
    "    \n",
    "    data_chunks.append(np.reshape(chunk_data_df.to_numpy(), (-1, 20, 13, 21)))\n",
    "    labels_chunks.append(chunk_labels_df)\n",
    "\n",
    "del data_chunk_list, labels_chunk_list, d, l, chunk_data_df, chunk_labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3acd6b-d1e4-4b65-bdd3-5c5a469686d2",
   "metadata": {},
   "source": [
    "## plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68942595-5b8e-4964-99f7-9cce1722058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "data = np.concatenate(data_chunks[:n], axis=0)\n",
    "labels = pd.concat(labels_chunks[:n], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b046c3-ac90-4054-939e-cb92b3e96819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(data[0].shape[0]):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.imshow(data[0][i], aspect='auto', cmap='viridis')\n",
    "    plt.title(f'Time slice {i+1}')\n",
    "    plt.xlabel('x [pixels]')\n",
    "    plt.ylabel('y [pixels]')\n",
    "    plt.colorbar(label='cluster charge')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc63eb-7053-48b9-8845-6ce74f589dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0_range = [(-1, 1), (-8, -6), (6, 8)]\n",
    "for r in y0_range:\n",
    "    data_y0range = data[(labels['y-local'] > r[0]) & (labels['y-local'] < r[1])]\n",
    "    labels_y0range = labels[(labels['y-local'] > r[0]) & (labels['y-local'] < r[1])]\n",
    "    \n",
    "    # take the last time slice, and sum over x to get y profile\n",
    "    y_profile = data_y0range[:, -1, :, :].sum(axis = -1)\n",
    "    \n",
    "    y_profile_low_pos = y_profile[(labels_y0range['pt'] < 0.2) & (labels_y0range['pt'] > 0)]\n",
    "    y_profile_low_neg = y_profile[(labels_y0range['pt'] > -0.2) & (labels_y0range['pt'] < 0)]\n",
    "    y_profile_high = y_profile[(labels_y0range['pt'] > 0.2) | (labels_y0range['pt'] < -0.2)]\n",
    "    \n",
    "    y_profile_low_pos_mean = y_profile_low_pos.mean(axis = 0)\n",
    "    y_profile_low_neg_mean = y_profile_low_neg.mean(axis = 0)\n",
    "    y_profile_high_mean = y_profile_high.mean(axis = 0)\n",
    "    \n",
    "    edges = np.arange(len(y_profile_low_pos_mean) + 1)\n",
    "    \n",
    "    y_profile_low_pos_mean = np.concatenate(([0], y_profile_low_pos_mean))\n",
    "    y_profile_low_neg_mean = np.concatenate(([0], y_profile_low_neg_mean))\n",
    "    y_profile_high_mean = np.concatenate(([0], y_profile_high_mean))\n",
    "    \n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.step(edges, y_profile_low_pos_mean/np.sum(y_profile_low_pos_mean), where='post', label = 'Low pT (pos)', color='red')\n",
    "    plt.step(edges, y_profile_low_neg_mean/np.sum(y_profile_low_neg_mean), where='post', label = 'Low pT (neg)', color='blue')\n",
    "    plt.step(edges, y_profile_high_mean/np.sum(y_profile_high_mean), where='post', label = 'High pT', color='black')\n",
    "    plt.title(f\"Mean cluster shape ({r[0]}mm < y0 < {r[1]}mm)\", size=10)\n",
    "    #plt.ylim(0, max(y_profile_low_neg_mean)*1.25)\n",
    "    plt.ylim(0, 0.35)\n",
    "    plt.xlabel(\"y [pixels]\")\n",
    "    plt.ylabel(\"Mean cluster charge\")\n",
    "    plt.xticks(np.arange(0, len(y_profile_low_pos_mean) + 1, 2))\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8409ea-ca00-4c25-9706-d2df9d2a5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9432c990-4020-4350-9ff1-b6284f8127f8",
   "metadata": {},
   "source": [
    "## prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1bd43-0558-422c-8880-0bd810ebc58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_chunks)):\n",
    "    data_chunks[i] = data_chunks[i][:, -1, :, :].sum(axis = -1)\n",
    "\n",
    "n = 16\n",
    "data = np.concatenate(data_chunks[:n], axis=0)\n",
    "del data_chunks\n",
    "labels = pd.concat(labels_chunks[:n], ignore_index=True)\n",
    "del labels_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1b7fe-bac9-4e5c-8aae-5135f7658275",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((labels['y-local'].to_numpy().reshape(-1, 1), data))\n",
    "del data\n",
    "\n",
    "pt_truth = labels['pt'].to_numpy()\n",
    "del labels\n",
    "\n",
    "Y = np.zeros((pt_truth.size, 3))\n",
    "Y[:, 2] = 1\n",
    "Y[np.where((pt_truth >= 0) & (pt_truth < 0.2))] = [1, 0, 0]\n",
    "Y[np.where((pt_truth < 0) & (pt_truth > -0.2))] = [0, 1, 0]\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(np.sum(Y, axis=0))\n",
    "print(pt_truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712eb24-28b9-4dee-a1b5-99b8878335b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class0_idx = np.where(Y[:, 0] == 1)[0]\n",
    "class1_idx = np.where(Y[:, 1] == 1)[0]\n",
    "class2_idx = np.where(Y[:, 2] == 1)[0]\n",
    "\n",
    "min_count = min(len(class0_idx), len(class1_idx), len(class2_idx))\n",
    "\n",
    "chosen_class0 = np.random.choice(class0_idx, min_count, replace=False)\n",
    "chosen_class1 = np.random.choice(class1_idx, min_count, replace=False)\n",
    "chosen_class2 = np.random.choice(class2_idx, min_count, replace=False)\n",
    "\n",
    "balanced_idx = np.concatenate([chosen_class0, chosen_class1, chosen_class2])\n",
    "\n",
    "np.random.shuffle(balanced_idx)\n",
    "\n",
    "X_balanced = X[balanced_idx]\n",
    "Y_balanced = Y[balanced_idx]\n",
    "pt_truth_balanced = pt_truth[balanced_idx]\n",
    "\n",
    "all_indices = np.arange(Y.shape[0])\n",
    "rest_idx = np.setdiff1d(all_indices, balanced_idx)\n",
    "\n",
    "X_rest = X[rest_idx]\n",
    "Y_rest = Y[rest_idx]\n",
    "pt_truth_rest = pt_truth[rest_idx]\n",
    "\n",
    "del X, Y, pt_truth\n",
    "\n",
    "print(X_balanced.shape)\n",
    "print(Y_balanced.shape)\n",
    "print(np.sum(Y_balanced, axis=0))\n",
    "print(pt_truth_balanced.shape)\n",
    "\n",
    "print(X_rest.shape)\n",
    "print(Y_rest.shape)\n",
    "print(np.sum(Y_rest, axis=0))\n",
    "print(pt_truth_rest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0829df2-c963-4680-ac00-81644d7e96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.6\n",
    "val_ratio = 0.05\n",
    "test_ratio = 1 - train_ratio - val_ratio\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X_balanced, Y_balanced, test_size = test_ratio, random_state = 42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size = val_ratio/(val_ratio + train_ratio), random_state = 42)\n",
    "\n",
    "pt_truth_train_val, pt_truth_test = train_test_split(pt_truth_balanced, test_size = test_ratio, random_state = 42)\n",
    "\n",
    "X_test = np.concatenate((X_test, X_rest), axis=0)\n",
    "Y_test = np.concatenate((Y_test, Y_rest), axis=0)\n",
    "pt_truth_test = np.concatenate((pt_truth_test, pt_truth_rest), axis=0)\n",
    "\n",
    "print('X_train shape: ' + str(X_train.shape))\n",
    "print('X_val   shape: ' + str(X_val.shape))\n",
    "print('X_test  shape: ' + str(X_test.shape))\n",
    "print('Y_train shape: ' + str(Y_train.shape))\n",
    "print('Y_val   shape: ' + str(Y_val.shape))\n",
    "print('Y_test  shape: ' + str(Y_test.shape))\n",
    "print('pt_truth_test  shape: ' + str(pt_truth_test.shape))\n",
    "\n",
    "del X_balanced, Y_balanced, pt_truth_balanced, X_rest, Y_rest, pt_truth_rest, X_train_val, Y_train_val, pt_truth_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea2ad4-2cfa-4f61-ae0b-64522001001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566c4c1-d156-47f0-bec2-069f424b1714",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_val.npy', X_val)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('Y_train.npy', Y_train)\n",
    "np.save('Y_val.npy', Y_val)\n",
    "np.save('Y_test.npy', Y_test)\n",
    "np.save('pt_truth_test.npy', pt_truth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec866e2-e527-4d45-ab90-221cbb2156c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train.npy', mmap_mode='r')\n",
    "X_val = np.load('X_val.npy', mmap_mode='r')\n",
    "X_test = np.load('X_test.npy', mmap_mode='r')\n",
    "Y_train = np.load('Y_train.npy', mmap_mode='r')\n",
    "Y_val = np.load('Y_val.npy', mmap_mode='r')\n",
    "Y_test = np.load('Y_test.npy', mmap_mode='r')\n",
    "pt_truth_test = np.load('pt_truth_test.npy', mmap_mode='r')\n",
    "\n",
    "print('X_train shape: ' + str(X_train.shape))\n",
    "print('X_val   shape: ' + str(X_val.shape))\n",
    "print('X_test  shape: ' + str(X_test.shape))\n",
    "print('Y_train shape: ' + str(Y_train.shape))\n",
    "print('Y_val   shape: ' + str(Y_val.shape))\n",
    "print('Y_test  shape: ' + str(Y_test.shape))\n",
    "print('pt_truth_test  shape: ' + str(pt_truth_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3dcd33-bf93-42f2-91fd-8d56797fb55c",
   "metadata": {},
   "source": [
    "## nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa29564-b382-4443-9d44-b530c0c604c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = quantized_bits(6, 0, alpha=1)\n",
    "quantized_relu = 'quantized_relu(6, 0)'\n",
    "\n",
    "x_input = keras.Input(shape=(14,), name='input')\n",
    "\n",
    "x = QDense(128, use_bias=True, name='dense1', kernel_quantizer=quantizer, bias_quantizer=quantizer)(x_input)\n",
    "x = QActivation(quantized_relu, name='relu1')(x)\n",
    "    \n",
    "x = QDense(3, use_bias=True, name='dense2', kernel_quantizer=quantizer, bias_quantizer=quantizer)(x)\n",
    "x = layers.Softmax(name='softmax')(x)\n",
    "\n",
    "model = keras.Model(x_input, x, name='model')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56450055-5f05-407b-b024-a3ee3bbd560f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data = (X_val, Y_val),\n",
    "                    epochs=100, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bec536-8ac6-4df9-a34d-d69a8178647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "axes.plot(history.history['loss'], label = 'train loss')\n",
    "axes.plot(history.history['val_loss'], label = 'val loss')\n",
    "axes.legend(loc = \"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770756f9-011f-40ab-a5cf-092bfec2c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a852df2-4b05-4d40-addd-2092ab3eac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_test, y_pred, labels):\n",
    "    for x, label in enumerate(labels):        \n",
    "        fpr, tpr, _ = roc_curve(y_test[:, x], y_pred[:, x])\n",
    "        plt.plot(fpr, tpr, label='{0}, {1:.1f}'.format(label, auc(fpr, tpr)*100.), linestyle='-')\n",
    "    #plt.semilogy()\n",
    "    #plt.semilogx()\n",
    "    plt.ylabel(\"Signal Efficiency\")\n",
    "    plt.xlabel(\"Background Efficiency\")\n",
    "    #plt.ylim(0.00001, 1)\n",
    "    #plt.xlim(0.00001, 1)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best', fontsize=10)  \n",
    "    \n",
    "plt.figure(figsize=(4, 4))\n",
    "plot_roc(Y_test, Y_pred, ['Low pT (pos)','Low pT (neg)','High pT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3c1bd-0409-406d-90ad-1205e0526eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bins = np.arange(-4.5, 4.5, 0.01)\n",
    "bin_centers = 0.5 * (custom_bins[:-1] + custom_bins[1:])\n",
    "\n",
    "indices_001 = np.argmax(Y_pred, axis=1) == 2\n",
    "\n",
    "total_counts, _ = np.histogram(pt_truth_test, bins=custom_bins)\n",
    "class_001_counts, _ = np.histogram(pt_truth_test[indices_001], bins=custom_bins)\n",
    "\n",
    "proportions = class_001_counts / total_counts\n",
    "proportions = np.nan_to_num(proportions)\n",
    "\n",
    "plt.scatter(bin_centers, proportions, marker='.', label='NN', alpha=0.5, color='blue')\n",
    "plt.axvline(x=0.2, color='grey', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=-0.2, color='grey', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel('True pT [GeV]')\n",
    "plt.ylabel('Fraction')\n",
    "plt.title('Fraction of clusters selected as having |pT| > 0.2 GeV')\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "#plt.yscale('log')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a075e5-a07e-473e-9fab-a00fbe22bc19",
   "metadata": {},
   "source": [
    "## pysr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beef3ba-6c4d-43b9-b17e-b720bb06d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysr import PySRRegressor\n",
    "\n",
    "model_sr = PySRRegressor(\n",
    "    model_selection = 'accuracy',\n",
    "    niterations = 80,\n",
    "    maxsize = 80,\n",
    "    select_k_features = 8,\n",
    "    #batching = True,\n",
    "    #batch_size = 1000,\n",
    "    binary_operators = [\n",
    "        '+', '*'\n",
    "                     ],\n",
    "    unary_operators = [\n",
    "        'sin',\n",
    "        'exp',\n",
    "        'tanh',\n",
    "    ],\n",
    "    nested_constraints = {\n",
    "        'sin':    {'sin': 0, 'exp': 0, 'tanh': 0, '*': 2},\n",
    "        'exp':    {'sin': 0, 'exp': 0, 'tanh': 0, '*': 2},\n",
    "        'tanh':   {'sin': 0, 'exp': 0, 'tanh': 0, '*': 2},\n",
    "        '*':      {'sin': 4, 'exp': 4, 'tanh': 4, '*': 4},\n",
    "    },\n",
    "    loss='loss(y, y_pred) = (y - y_pred)^2',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f862b63-4ca5-43b3-917d-dc9c137b5576",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sr.fit(X_train[:40000], Y_train[:40000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aaa57f-630b-4d99-b109-d3494907d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysr import PySRRegressor\n",
    "model_sr = PySRRegressor.from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7a0b0-51c7-4ac1-a5a6-e54d7cddbd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"Eqn {} = \".format(i)+str(model_sr.sympy()[i])+\"\\n------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d28e44-2bc2-4284-a8f5-91c1b6fa2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_sr = model_sr.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(Y_test, axis=1), np.argmax(Y_pred_sr, axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8892e-0ef5-480a-87df-5c357250ba15",
   "metadata": {},
   "source": [
    "## symbolnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a597f43-97e1-41a0-a7b2-2bd610f94b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_operation(tf_or_sympy, operator, x, y=None):\n",
    "    # unary operators\n",
    "    if operator == 'identity':\n",
    "        output = x\n",
    "    elif operator == 'sin':\n",
    "        output = tf.sin(x) if tf_or_sympy == 'tf' else sympy.sin(x)\n",
    "    elif operator == 'cos':\n",
    "        output = tf.cos(x) if tf_or_sympy == 'tf' else sympy.cos(x)\n",
    "    elif operator == 'exp':\n",
    "        output = tf.exp(x) if tf_or_sympy == 'tf' else sympy.exp(x)\n",
    "    elif operator == 'gauss':\n",
    "        output = tf.exp(-x**2) if tf_or_sympy == 'tf' else sympy.exp(-x**2)\n",
    "    elif operator == 'sinh':\n",
    "        output = tf.sinh(x) if tf_or_sympy == 'tf' else sympy.sinh(x)\n",
    "    elif operator == 'cosh':\n",
    "        output = tf.cosh(x) if tf_or_sympy == 'tf' else sympy.cosh(x)\n",
    "    elif operator == 'tanh':\n",
    "        output = tf.tanh(x) if tf_or_sympy == 'tf' else sympy.tanh(x)\n",
    "    elif operator == 'square':\n",
    "        output = x**2 if tf_or_sympy == 'tf' else x**2\n",
    "    elif operator == 'cube':\n",
    "        output = x**3 if tf_or_sympy == 'tf' else x**3\n",
    "    elif operator == 'log':\n",
    "        output = tf.math.log(0.001 + tf.abs(x)) if tf_or_sympy == 'tf' else sympy.log(0.001 + sympy.Abs(x))\n",
    "        #output = tf.math.log(tf.abs(x)) if tf_or_sympy == 'tf' else sympy.log(sympy.Abs(x))\n",
    "    # binary operators\n",
    "    elif operator == '+':\n",
    "        output = x + y\n",
    "    elif operator == '*':\n",
    "        output = x * y\n",
    "    elif operator == 'pow':\n",
    "        output = x ** y\n",
    "    elif operator == '/':\n",
    "        output = x / (0.001 + tf.abs(y)) if tf_or_sympy == 'tf' else x / (0.001 + sympy.Abs(y))\n",
    "        #output = x / y\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8acb38b-7d78-448d-b5ce-c8ca999c6446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step function for masking in the forward pass\n",
    "# custom grad is an estimator of derivative of the step function to avoid vanishing gradient\n",
    "@tf.custom_gradient\n",
    "def step_func(x):\n",
    "    func = tf.where(x > 0., 1., 0.)\n",
    "    def grad(upstream):\n",
    "        a = 5.\n",
    "        return upstream * a * tf.exp(-a*x) / (1 + tf.exp(-a*x))**2\n",
    "    return func, grad\n",
    "\n",
    "# dynamic pruning of input features\n",
    "# define one auxiliary weight and one untrainable threshold per input feature\n",
    "class Input_sparsity(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # auxiliary weight is untrainable and fixed at 1\n",
    "        self.aux_w = self.add_weight(name='weight',\n",
    "                                     shape=(input_shape[-1],),\n",
    "                                     initializer='ones',\n",
    "                                     trainable=False)\n",
    "        # threshold is trainable, initialized at 0, and bounded in [0,1]\n",
    "        self.aux_w_t = self.add_weight(name='threshold',\n",
    "                                     shape=(input_shape[-1],),\n",
    "                                     initializer='zeros',\n",
    "                                     constraint=lambda x: tf.clip_by_value(x, 0., 1.),\n",
    "                                     trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_masks = step_func(self.aux_w - self.aux_w_t)\n",
    "        return tf.multiply(inputs, input_masks)\n",
    "\n",
    "# definition of symbolic layer (usual weights and biases, plus unary/binary operators as activations)\n",
    "# define trainable thresholds for weights/biases/unary/binary\n",
    "class Symbolic_Layer(Layer):\n",
    "    def __init__(self, operators, num_operators):\n",
    "        super().__init__()\n",
    "        self.operators = operators\n",
    "        self.num_operators = num_operators\n",
    "        self.units = self.num_operators[0] + 2*self.num_operators[1]\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # usual model weight w\n",
    "        self.w = self.add_weight(name='weight',\n",
    "                                 shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        # usual bias b\n",
    "        self.b = self.add_weight(name='bias',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        # auxiliary weight for unary operator, to be used for operator pruning\n",
    "        # untrainable and fixed at 1\n",
    "        self.aux_unary = self.add_weight(name='unary',\n",
    "                                         shape=(self.num_operators[0],),\n",
    "                                         initializer='ones',\n",
    "                                         trainable=False)\n",
    "        # auxiliary weight for binary operator, to be used for operator pruning\n",
    "        # untrainable and fixed at 1\n",
    "        if self.num_operators[1] > 0:\n",
    "            self.aux_binary = self.add_weight(name='binary',\n",
    "                                              shape=(self.num_operators[1],),\n",
    "                                              initializer='ones',\n",
    "                                              trainable=False)\n",
    "        # threshold for model weight\n",
    "        # trainable, initialized at 0, unbounded since model weight is unbounded\n",
    "        self.aux_w_t = self.add_weight(name='weight_threshold',\n",
    "                                       shape=(input_shape[-1], self.units),\n",
    "                                       initializer='zeros',\n",
    "                                       constraint=lambda x: tf.abs(x),\n",
    "                                       trainable=True)\n",
    "        # threshold for bias term\n",
    "        # trainable, initialized at 0, unbounded since model weight is unbounded\n",
    "        self.aux_b_t = self.add_weight(name='bias_threshold',\n",
    "                                       shape=(self.units,),\n",
    "                                       initializer='zeros',\n",
    "                                       constraint=lambda x: tf.abs(x),\n",
    "                                       trainable=True)\n",
    "        # threshold for unary operator\n",
    "        # trainable, initialized at 0, bounded in [0,1]\n",
    "        self.aux_unary_t = self.add_weight(name='unary_threshold',\n",
    "                                          shape=(self.num_operators[0],),\n",
    "                                          initializer='zeros',\n",
    "                                          constraint=lambda x: tf.clip_by_value(x, 0., 1.),\n",
    "                                          trainable=True)\n",
    "        # threshold for binary operator\n",
    "        # trainable, initialized at 0, bounded in [0,1]\n",
    "        if self.num_operators[1] > 0:\n",
    "            self.aux_binary_t = self.add_weight(name='binary_threshold',\n",
    "                                                shape=(self.num_operators[1],),\n",
    "                                                initializer='zeros',\n",
    "                                                constraint=lambda x: tf.clip_by_value(x, 0., 1.),\n",
    "                                                trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # linear transformation in the forward pass\n",
    "        # weights and biases are replaced by the step_functioned version\n",
    "        # so weight is masked whenever its threshold is higher\n",
    "        w_masks = step_func(tf.abs(self.w) - self.aux_w_t)\n",
    "        b_masks = step_func(tf.abs(self.b) - self.aux_b_t)\n",
    "        linear_output = tf.matmul(inputs, tf.multiply(self.w, w_masks)) + tf.multiply(self.b, b_masks)\n",
    "        \n",
    "        # activation by unary/binary operator\n",
    "        symbolic_output = []\n",
    "        \n",
    "        # loop over number of unary operators in a symbolic layer\n",
    "        for i in range(self.num_operators[0]):\n",
    "            # an unary operator is \"pruned\" (becomes identity map) whenever the threshold is higher than its auxiliary weight\n",
    "            unary_mask = step_func(self.aux_unary - self.aux_unary_t)[i]\n",
    "            idx = np.mod(i, len(self.operators[0]))\n",
    "            unary_operation = (unary_mask * math_operation('tf',self.operators[0][idx],linear_output[:, i:i+1]) +\n",
    "                               (1.0 - unary_mask) * math_operation('tf','identity',linear_output[:, i:i+1]))\n",
    "            symbolic_output.append(unary_operation)\n",
    "            \n",
    "        # loop over number of binary operators in a symbolic layer\n",
    "        for i in range(self.num_operators[0], self.num_operators[0] + 2*self.num_operators[1], 2):\n",
    "            # a binary operator is \"pruned\" (becomes addition) whenever the threshold is higher than its auxiliary weight\n",
    "            j = int((i - self.num_operators[0])/2)\n",
    "            binary_mask = step_func(self.aux_binary - self.aux_binary_t)[j]\n",
    "            idx = np.mod(j, len(self.operators[1]))\n",
    "            binary_operation = (binary_mask * math_operation('tf',self.operators[1][idx],linear_output[:, i:i+1],linear_output[:, i+1:i+2]) +\n",
    "                                (1.0 - binary_mask) * math_operation('tf','+',linear_output[:, i:i+1],linear_output[:, i+1:i+2]))\n",
    "            symbolic_output.append(binary_operation)\n",
    "        \n",
    "        symbolic_output = tf.concat(symbolic_output, axis=1)\n",
    "        return symbolic_output\n",
    "    \n",
    "def create_model(model_dim, operators, num_operators):\n",
    "    input_dim, num_hidden_layers, output_dim = model_dim\n",
    "    layers = []\n",
    "    \n",
    "    # input layer\n",
    "    layers.append(Input(shape=(input_dim,)))\n",
    "    layers.append(Input_sparsity()(layers[-1]))\n",
    "    \n",
    "    # hidden symbolic layers\n",
    "    for i in range(num_hidden_layers):\n",
    "        layers.append(Symbolic_Layer(operators=operators[i],\n",
    "                                     num_operators=num_operators[i])(layers[-1]))\n",
    "    \n",
    "    # output layer\n",
    "    layers.append(Symbolic_Layer(operators=[['identity'], [None]],\n",
    "                                 num_operators=[output_dim, 0])(layers[-1]))\n",
    "    \n",
    "    model = keras.Model(inputs=layers[0], outputs=layers[-1], name='model')\n",
    "    return model, model_dim, operators, num_operators\n",
    "\n",
    "input_dim  = X_train.shape[1]\n",
    "num_hidden = 1\n",
    "output_dim = Y_train.shape[1]\n",
    "model_dim = [input_dim, num_hidden, output_dim]\n",
    "\n",
    "# operator choices per hidden symbolic layer\n",
    "operators = [\n",
    "    [['sin','tanh','gauss'], ['*']], # 1st symbolic layer [[unary], [binary]]\n",
    "    #[['sin','tanh','gauss'], ['*']], # 2nd ...\n",
    "            ]\n",
    "\n",
    "# number of unary and binary operators per hidden symbolic layer\n",
    "num_operators = [\n",
    "    [20, 10], # 1st symbolic layer [num_unary, num_binary]\n",
    "    #[16, 8], # 2nd ...\n",
    "]\n",
    "\n",
    "model = create_model(model_dim=model_dim,\n",
    "                     operators=operators,\n",
    "                     num_operators=num_operators)\n",
    "model[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221fe0e-a49f-41c7-b73a-c960855f4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralSR(keras.Model):\n",
    "    def __init__(self, model, alpha_sparsity_input, alpha_sparsity_model, alpha_sparsity_unary, alpha_sparsity_binary):\n",
    "        super().__init__()\n",
    "        self.model, self.model_dim, self.operators, self.num_operators = model\n",
    "        self.alpha_sparsity_input = alpha_sparsity_input\n",
    "        self.alpha_sparsity_model = alpha_sparsity_model\n",
    "        self.alpha_sparsity_unary = alpha_sparsity_unary\n",
    "        self.alpha_sparsity_binary = alpha_sparsity_binary\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.regression_loss_tracker = keras.metrics.Mean(name=\"regression_loss\")\n",
    "        self.threshold_input_reg_loss_tracker = keras.metrics.Mean(name=\"threshold_input_reg_loss\")\n",
    "        self.threshold_input_mean_tracker = keras.metrics.Mean(name=\"threshold_input_mean\")\n",
    "        self.threshold_model_reg_loss_tracker = keras.metrics.Mean(name=\"threshold_model_reg_loss\")\n",
    "        self.threshold_model_mean_tracker = keras.metrics.Mean(name=\"threshold_model_mean\")\n",
    "        self.threshold_unary_reg_loss_tracker = keras.metrics.Mean(name=\"threshold_unary_reg_loss\")\n",
    "        self.threshold_unary_mean_tracker = keras.metrics.Mean(name=\"threshold_unary_mean\")\n",
    "        self.threshold_binary_reg_loss_tracker = keras.metrics.Mean(name=\"threshold_binary_reg_loss\")\n",
    "        self.threshold_binary_mean_tracker = keras.metrics.Mean(name=\"threshold_binary_mean\")\n",
    "        self.weight_input_mean_tracker = keras.metrics.Mean(name=\"weight_input_mean\")\n",
    "        self.weight_model_mean_tracker = keras.metrics.Mean(name=\"weight_model_mean\")\n",
    "        self.weight_unary_mean_tracker = keras.metrics.Mean(name=\"weight_unary_mean\")\n",
    "        self.weight_binary_mean_tracker = keras.metrics.Mean(name=\"weight_binary_mean\")\n",
    "        self.sparsity_input_tracker = keras.metrics.Mean(name=\"sparsity_input\")\n",
    "        self.sparsity_model_tracker = keras.metrics.Mean(name=\"sparsity_model\")\n",
    "        self.sparsity_unary_tracker = keras.metrics.Mean(name=\"sparsity_unary\")\n",
    "        self.sparsity_binary_tracker = keras.metrics.Mean(name=\"sparsity_binary\")\n",
    "        self.accuracy_tracker = keras.metrics.Accuracy(name=\"accuracy\")\n",
    "    \n",
    "    def get_hyperparameters(self):\n",
    "        return self.model_dim, self.operators, self.num_operators\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.regression_loss_tracker,\n",
    "            self.threshold_input_reg_loss_tracker,\n",
    "            self.threshold_input_mean_tracker,\n",
    "            self.threshold_model_reg_loss_tracker,\n",
    "            self.threshold_model_mean_tracker,\n",
    "            self.threshold_unary_reg_loss_tracker,\n",
    "            self.threshold_unary_mean_tracker,\n",
    "            self.threshold_binary_reg_loss_tracker,\n",
    "            self.threshold_binary_mean_tracker,\n",
    "            self.weight_input_mean_tracker,\n",
    "            self.weight_model_mean_tracker,\n",
    "            self.weight_unary_mean_tracker,\n",
    "            self.weight_binary_mean_tracker,\n",
    "            self.sparsity_input_tracker,\n",
    "            self.sparsity_model_tracker,\n",
    "            self.sparsity_unary_tracker,\n",
    "            self.sparsity_binary_tracker,\n",
    "            self.accuracy_tracker\n",
    "        ]\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.model(x)\n",
    "            \n",
    "            # base training loss (MSE)\n",
    "            regression_loss = tf.reduce_mean(tf.reduce_sum(tf.cast((y-y_pred)**2,dtype=tf.float64), axis=1))\n",
    "            \n",
    "            # the followings calculate sparsity levels at train steps\n",
    "            # since total loss = MSE + sparsity regularization terms\n",
    "            # where sparsity regularization terms depend on sparsity levels\n",
    "            \n",
    "            # call auxiliary weights and thresholds for inputs\n",
    "            w_input = self.non_trainable_weights[0]\n",
    "            w_t_input = self.trainable_weights[0]\n",
    "            num_input_masks = tf.reduce_sum(tf.cast(tf.where(w_input - w_t_input > 0., 0., 1.), dtype=tf.float64))\n",
    "            num_input_weights = tf.size(w_input, out_type=tf.float64)\n",
    "            sparsity_input = num_input_masks/num_input_weights\n",
    "            weight_input_mean = tf.reduce_sum(tf.cast(tf.abs(w_input),dtype=tf.float64))/num_input_weights\n",
    "            \n",
    "            # calculate input sparsity\n",
    "            t_input_sum = tf.reduce_sum(tf.cast(w_t_input,dtype=tf.float64))\n",
    "            t_input_dim = num_input_weights\n",
    "            threshold_input_mean = t_input_sum/t_input_dim\n",
    "            \n",
    "            # call model weights and thresholds for all hidden symbolic layers\n",
    "            # then calculate model weight (weight+bias) sparsity\n",
    "            num_model_masks = 0.\n",
    "            num_model_weights = 0.\n",
    "            weight_model_mean = 0.\n",
    "            t_model_sum = 0.\n",
    "            t_model_dim = 0.\n",
    "            sum_exp_t = 0.\n",
    "            for i in range(self.model_dim[1]+1):\n",
    "                w_model = self.trainable_weights[1+6*i]\n",
    "                b_model = self.trainable_weights[1+6*i+1]\n",
    "                w_t_model = self.trainable_weights[1+6*i+2]\n",
    "                b_t_model = self.trainable_weights[1+6*i+3]\n",
    "                sum_exp_t += tf.reduce_sum(tf.exp(-tf.cast(w_t_model,dtype=tf.float64)))\n",
    "                sum_exp_t += tf.reduce_sum(tf.exp(-tf.cast(b_t_model,dtype=tf.float64)))\n",
    "                num_model_masks += tf.reduce_sum(tf.cast(tf.where(-w_t_model + tf.abs(w_model) > 0., 0., 1.),dtype=tf.float64))\n",
    "                num_model_masks += tf.reduce_sum(tf.cast(tf.where(-b_t_model + tf.abs(b_model) > 0., 0., 1.),dtype=tf.float64))\n",
    "                num_model_weights += tf.size(w_model, out_type=tf.float64)\n",
    "                num_model_weights += tf.size(b_model, out_type=tf.float64)\n",
    "                weight_model_mean += tf.reduce_sum(tf.cast(tf.abs(w_model),dtype=tf.float64)) + tf.reduce_sum(tf.cast(tf.abs(b_model),dtype=tf.float64))\n",
    "                t_model_dim += tf.size(w_t_model, out_type=tf.float64) + tf.size(b_t_model, out_type=tf.float64)\n",
    "                t_model_sum += tf.reduce_sum(tf.cast(w_t_model,dtype=tf.float64)) + tf.reduce_sum(tf.cast(b_t_model,dtype=tf.float64))\n",
    "            sparsity_model = num_model_masks/num_model_weights\n",
    "            weight_model_mean = weight_model_mean/num_model_weights\n",
    "\n",
    "            threshold_model_mean = t_model_sum/t_model_dim\n",
    "            \n",
    "            # call auxiliary weights and thresholds for unary and binary operators\n",
    "            num_unary_masks = 0.\n",
    "            num_unary_weights = 0.\n",
    "            weight_unary_mean = 0.\n",
    "            num_binary_masks = 0.\n",
    "            num_binary_weights = 0.\n",
    "            weight_binary_mean = 0.\n",
    "            t_u_sum = 0.\n",
    "            t_u_dim = 0.\n",
    "            t_b_sum = 0.\n",
    "            t_b_dim = 0.\n",
    "            for i in range(self.model_dim[1]):\n",
    "                u = self.non_trainable_weights[2*i+1]\n",
    "                u_t = self.trainable_weights[1+6*i+4]\n",
    "                num_unary_masks += tf.reduce_sum(tf.cast(tf.where(u - u_t > 0., 0., 1.),dtype=tf.float64))\n",
    "                num_unary_weights += tf.size(u, out_type=tf.float64)\n",
    "                weight_unary_mean += tf.reduce_sum(tf.cast(u,dtype=tf.float64))\n",
    "                t_u_dim += tf.size(u_t, out_type=tf.float64)\n",
    "                t_u_sum += tf.reduce_sum(tf.cast(u_t,dtype=tf.float64))\n",
    "                \n",
    "                b = self.non_trainable_weights[2*i+2]\n",
    "                b_t = self.trainable_weights[1+6*i+5]\n",
    "                num_binary_masks += tf.reduce_sum(tf.cast(tf.where(b - b_t > 0., 0., 1.),dtype=tf.float64))\n",
    "                num_binary_weights += tf.size(b, out_type=tf.float64)\n",
    "                weight_binary_mean += tf.reduce_sum(tf.cast(b,dtype=tf.float64))\n",
    "                t_b_dim += tf.size(b_t, out_type=tf.float64)\n",
    "                t_b_sum += tf.reduce_sum(tf.cast(b_t,dtype=tf.float64))\n",
    "                    \n",
    "            # calculate sparsity levels for unary and binary operators\n",
    "            sparsity_unary = num_unary_masks/num_unary_weights\n",
    "            sparsity_binary = num_binary_masks/num_binary_weights\n",
    "            weight_unary_mean = weight_unary_mean/num_unary_weights\n",
    "            weight_binary_mean = weight_binary_mean/num_binary_weights\n",
    "            \n",
    "            threshold_unary_mean = t_u_sum/t_u_dim\n",
    "            threshold_binary_mean = t_b_sum/t_b_dim\n",
    "            \n",
    "            # sparsity regularization terms\n",
    "            threshold_input_reg_loss = regression_loss*tf.exp(-threshold_input_mean)\n",
    "            threshold_model_reg_loss = regression_loss*sum_exp_t/num_model_weights\n",
    "            threshold_unary_reg_loss = regression_loss*tf.exp(-threshold_unary_mean)\n",
    "            threshold_binary_reg_loss = regression_loss*tf.exp(-threshold_binary_mean)\n",
    "            # additional decay factor\n",
    "            def reg(s, s_t, d):\n",
    "                return tf.exp(-(s_t/(s_t-tf.minimum(s, s_t)))**d+1.)\n",
    "            threshold_input_reg_loss *= reg(sparsity_input, self.alpha_sparsity_input, 0.01)\n",
    "            threshold_model_reg_loss *= reg(sparsity_model, self.alpha_sparsity_model, 0.01)\n",
    "            threshold_unary_reg_loss *= reg(sparsity_unary, self.alpha_sparsity_unary, 0.01)\n",
    "            threshold_binary_reg_loss *= reg(sparsity_binary, self.alpha_sparsity_binary, 0.01)\n",
    "            \n",
    "            # total loss\n",
    "            total_loss = regression_loss + (threshold_model_reg_loss + \n",
    "                                            threshold_input_reg_loss + \n",
    "                                            threshold_unary_reg_loss +\n",
    "                                            threshold_binary_reg_loss)\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "                \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.regression_loss_tracker.update_state(regression_loss)\n",
    "        self.threshold_input_reg_loss_tracker.update_state(threshold_input_reg_loss)\n",
    "        self.threshold_input_mean_tracker.update_state(threshold_input_mean)\n",
    "        self.threshold_model_reg_loss_tracker.update_state(threshold_model_reg_loss)\n",
    "        self.threshold_model_mean_tracker.update_state(threshold_model_mean)\n",
    "        self.threshold_unary_reg_loss_tracker.update_state(threshold_unary_reg_loss)\n",
    "        self.threshold_unary_mean_tracker.update_state(threshold_unary_mean)\n",
    "        self.threshold_binary_reg_loss_tracker.update_state(threshold_binary_reg_loss)\n",
    "        self.threshold_binary_mean_tracker.update_state(threshold_binary_mean)\n",
    "        self.weight_model_mean_tracker.update_state(weight_model_mean)\n",
    "        self.weight_input_mean_tracker.update_state(weight_input_mean)\n",
    "        self.weight_unary_mean_tracker.update_state(weight_unary_mean)\n",
    "        self.weight_binary_mean_tracker.update_state(weight_binary_mean)\n",
    "        self.accuracy_tracker.update_state(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1))\n",
    "        self.sparsity_input_tracker.update_state(sparsity_input)\n",
    "        self.sparsity_model_tracker.update_state(sparsity_model)\n",
    "        self.sparsity_unary_tracker.update_state(sparsity_unary)\n",
    "        self.sparsity_binary_tracker.update_state(sparsity_binary)\n",
    "        return {\n",
    "            'loss': self.total_loss_tracker.result(),\n",
    "            'regression_loss': self.regression_loss_tracker.result(),\n",
    "            'threshold_input_reg_loss': self.threshold_input_reg_loss_tracker.result(),\n",
    "            'threshold_input_mean': self.threshold_input_mean_tracker.result(),\n",
    "            'threshold_model_reg_loss': self.threshold_model_reg_loss_tracker.result(),\n",
    "            'threshold_model_mean': self.threshold_model_mean_tracker.result(),\n",
    "            'threshold_unary_reg_loss': self.threshold_unary_reg_loss_tracker.result(),\n",
    "            'threshold_unary_mean': self.threshold_unary_mean_tracker.result(),\n",
    "            'threshold_binary_reg_loss': self.threshold_binary_reg_loss_tracker.result(),\n",
    "            'threshold_binary_mean': self.threshold_binary_mean_tracker.result(),\n",
    "            'weight_model_mean': self.weight_model_mean_tracker.result(),\n",
    "            #'weight_input_mean': self.weight_input_mean_tracker.result(),\n",
    "            #'weight_unary_mean': self.weight_unary_mean_tracker.result(),\n",
    "            #'weight_binary_mean': self.weight_binary_mean_tracker.result(),\n",
    "            'sparsity_input': self.sparsity_input_tracker.result(),\n",
    "            'sparsity_model': self.sparsity_model_tracker.result(),\n",
    "            'sparsity_unary': self.sparsity_unary_tracker.result(),\n",
    "            'sparsity_binary': self.sparsity_binary_tracker.result(),\n",
    "            'accuracy': self.accuracy_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202acc3-8f19-4c5f-a389-33763a2eb2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsr = neuralSR(model,\n",
    "               # set target sparsity level per pruning type\n",
    "               alpha_sparsity_input=0.2,\n",
    "               alpha_sparsity_model=0.5,\n",
    "               alpha_sparsity_unary=0.4,\n",
    "               alpha_sparsity_binary=0.4)\n",
    "nsr.compile(optimizer=keras.optimizers.Adam(learning_rate=0.003))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601f5ec-4b9d-4c21-89cd-3293d50ffdd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = nsr.fit(X_train, Y_train, epochs=40, batch_size=256)\n",
    "history = dict()\n",
    "for key in h.history.keys():\n",
    "    values = []\n",
    "    values += h.history[key]\n",
    "    history[key] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2fa48f-f848-4645-811b-6d474dcdcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_axis_title=10\n",
    "size_axis_label=8\n",
    "size_legend=7\n",
    "plt.figure(figsize = (7,7))\n",
    "axes = plt.subplot(2,2,1)\n",
    "axes.set_ylim([-0.05,1.1])\n",
    "plt.xticks(fontsize = size_axis_label) \n",
    "plt.yticks(fontsize = size_axis_label) \n",
    "axes.plot(history['accuracy'], label='Accuracy', linestyle='solid', c='r')\n",
    "axes.plot(history['sparsity_model'], label='Sparsity (weight)', linestyle='solid', c='g')\n",
    "axes.plot(history['sparsity_input'], label='Sparsity (input)', linestyle='solid', c='b')\n",
    "axes.plot(history['sparsity_unary'], label='Sparsity (unary)', linestyle='solid', c='orange')\n",
    "axes.plot(history['sparsity_binary'], label='Sparsity (binary)', linestyle='solid', c='brown')\n",
    "axes.set_xlabel('Epoch', size=size_axis_title, loc='right')\n",
    "axes.set_ylabel('Metric', size=size_axis_title, loc='top')\n",
    "axes.legend(loc = 'best', frameon = False, fontsize = size_legend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc4fc3c-253b-4693-bab6-3e1709831147",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = nsr.model.predict(X_test)\n",
    "print(\"Accuracy = {}\".format(accuracy_score(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24715d3f-faa5-4b07-8312-95582b5e8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_test, y_pred, labels):\n",
    "    for x, label in enumerate(labels):        \n",
    "        fpr, tpr, _ = roc_curve(y_test[:, x], y_pred[:, x])\n",
    "        plt.plot(fpr, tpr, label='{0}, {1:.1f}'.format(label, auc(fpr, tpr)*100.), linestyle='-')\n",
    "    #plt.semilogy()\n",
    "    #plt.semilogx()\n",
    "    plt.ylabel(\"Signal Efficiency\")\n",
    "    plt.xlabel(\"Background Efficiency\")\n",
    "    #plt.ylim(0.00001, 1)\n",
    "    #plt.xlim(0.00001, 1)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best', fontsize=10)  \n",
    "    \n",
    "plt.figure(figsize=(4, 4))\n",
    "plot_roc(Y_test, Y_pred, ['Low pT (pos)','Low pT (neg)','High pT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382de1de-783e-47c3-8e4e-ed39556a8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bins = np.arange(-4.5, 4.5, 0.01)\n",
    "bin_centers = 0.5 * (custom_bins[:-1] + custom_bins[1:])\n",
    "\n",
    "indices_001 = np.argmax(Y_pred, axis=1) == 2\n",
    "\n",
    "total_counts, _ = np.histogram(pt_truth_test, bins=custom_bins)\n",
    "class_001_counts, _ = np.histogram(pt_truth_test[indices_001], bins=custom_bins)\n",
    "\n",
    "proportions = class_001_counts / total_counts\n",
    "proportions = np.nan_to_num(proportions)\n",
    "\n",
    "plt.scatter(bin_centers, proportions, marker='.', label='sr', alpha=0.5, color='blue')\n",
    "plt.axvline(x=0.2, color='grey', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=-0.2, color='grey', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel('True pT [GeV]')\n",
    "plt.ylabel('Fraction')\n",
    "plt.title('Fraction of clusters selected as having |pT| > 0.2 GeV')\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "#plt.yscale('log')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570d705-5cb4-4f72-9620-056c8a8e0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set significant digits for expression display\n",
    "significant_digits = 2\n",
    "\n",
    "# use sympy to expand the trained model\n",
    "def get_expressions(neuralSR):\n",
    "    model_dim, operators, num_operators = neuralSR.get_hyperparameters()\n",
    "    input_dim, num_hidden_layers, output_dim = model_dim\n",
    "    \n",
    "    x=[]\n",
    "    for i in range(input_dim):\n",
    "        x.append(sympy.Symbol('x{}'.format(i)))\n",
    "    x_masked = sympy.Matrix([x])\n",
    "    \n",
    "    w_input = neuralSR.model.layers[1].get_weights()[1]\n",
    "    t_input = neuralSR.model.layers[1].get_weights()[0]\n",
    "    num_input_masks = tf.reduce_sum(tf.cast(tf.where(w_input-t_input>0., 0., 1.),dtype=tf.float64))\n",
    "    num_input_weights = tf.size(w_input, out_type=tf.float64)\n",
    "    \n",
    "    sparsity_input = num_input_masks/num_input_weights\n",
    "    \n",
    "    w_input_masked = sympy.Matrix(tf.where(w_input-t_input>0., w_input, 0.))\n",
    "    x_print = np.multiply(sympy.Transpose(w_input_masked), x_masked)\n",
    "    x_masked = np.multiply(sympy.Transpose(w_input_masked), x_masked)\n",
    "\n",
    "    print('Remaining Inputs after pruning: {}\\n'.format(str(x_masked).replace('1.0*','')))\n",
    "    \n",
    "    num_masks = 0.\n",
    "    num_weights = 0.\n",
    "    \n",
    "    num_unary_masks = 0.\n",
    "    num_binary_masks = 0.\n",
    "    num_unary = 0.\n",
    "    num_binary = 0.\n",
    "    \n",
    "    for i in range(num_hidden_layers+1):\n",
    "        w = neuralSR.model.layers[i+2].get_weights()[0]\n",
    "        b = neuralSR.model.layers[i+2].get_weights()[1]\n",
    "        w_t = neuralSR.model.layers[i+2].get_weights()[2]\n",
    "        b_t = neuralSR.model.layers[i+2].get_weights()[3]\n",
    "        \n",
    "        num_masks += tf.reduce_sum(tf.cast(tf.where(tf.abs(w) - w_t > 0., 0., 1.),dtype=tf.float64))\n",
    "        num_masks += tf.reduce_sum(tf.cast(tf.where(tf.abs(b) - b_t > 0., 0., 1.),dtype=tf.float64))\n",
    "        num_weights += tf.size(w, out_type=tf.float64)\n",
    "        num_weights += tf.size(b, out_type=tf.float64)\n",
    "        \n",
    "        w_masked = sympy.Matrix(tf.where(tf.abs(w) - w_t > 0., w, 0.))\n",
    "        b_masked = sympy.Transpose(sympy.Matrix(tf.where(tf.abs(b) - b_t > 0., b, 0.)))\n",
    "        \n",
    "        x_masked = (x_masked * w_masked + b_masked).evalf(significant_digits)\n",
    "        if i < num_hidden_layers:\n",
    "            unary = neuralSR.model.layers[i+2].get_weights()[6]\n",
    "            unary_t = neuralSR.model.layers[i+2].get_weights()[4]\n",
    "            binary = neuralSR.model.layers[i+2].get_weights()[7]\n",
    "            binary_t = neuralSR.model.layers[i+2].get_weights()[5]\n",
    "            num_unary_masks += tf.reduce_sum(tf.cast(tf.where(unary - unary_t > 0., 0., 1.),dtype=tf.float64))\n",
    "            num_binary_masks += tf.reduce_sum(tf.cast(tf.where(binary - binary_t > 0., 0., 1.),dtype=tf.float64))\n",
    "            num_unary += tf.size(unary, out_type=tf.float64)\n",
    "            num_binary += tf.size(binary, out_type=tf.float64)\n",
    "        elif i == num_hidden_layers:\n",
    "            num_operators.append([output_dim, 0])\n",
    "            operators.append([['identity'], [None]])\n",
    "            \n",
    "        y_masked = sympy.zeros(1, num_operators[i][0] + num_operators[i][1])\n",
    "        \n",
    "        unary_mask = sympy.Matrix(tf.where(unary - unary_t > 0., 1., 0.))\n",
    "        binary_mask = sympy.Matrix(tf.where(binary - binary_t > 0., 1., 0.))\n",
    "        for j in range(num_operators[i][0]):\n",
    "            idx = np.mod(j, len(operators[i][0]))\n",
    "            if i < num_hidden_layers:\n",
    "                y_masked[0,j] = (unary_mask[j] * math_operation('sympy',operators[i][0][idx],x_masked[0,j]) +\n",
    "                             (1.0 - unary_mask[j]) * math_operation('sympy','identity',x_masked[0,j]))\n",
    "            elif i == num_hidden_layers:\n",
    "                y_masked[0,j] = x_masked[0,j]\n",
    "        for j in range(num_operators[i][1]):\n",
    "            idx = np.mod(j, len(operators[i][1]))\n",
    "            y_masked[0,num_operators[i][0]+j] = (binary_mask[j] * math_operation('sympy',operators[i][1][idx],x_masked[0,num_operators[i][0]+2*j],x_masked[0,num_operators[i][0]+2*j+1]) +\n",
    "                                              (1.0 - binary_mask[j]) * math_operation('sympy','+',x_masked[0,num_operators[i][0]+2*j],x_masked[0,num_operators[i][0]+2*j+1]))\n",
    "        x_masked = y_masked.evalf(significant_digits)\n",
    "    \n",
    "    sparsity_model = num_masks/num_weights\n",
    "    sparsity_unary = num_unary_masks/num_unary\n",
    "    sparsity_binary = num_binary_masks/num_binary\n",
    "    \n",
    "    complexity = []\n",
    "    for j in range(len(x_masked)):\n",
    "        c = 0\n",
    "        for tree_node in sympy.preorder_traversal(x_masked[j]):\n",
    "            c += 1\n",
    "        complexity.append(c)\n",
    "    \n",
    "    return x_masked, complexity, sparsity_input, sparsity_model, sparsity_unary, sparsity_binary\n",
    "  \n",
    "expressions_masked, complexity, sparsity_input, sparsity_model, sparsity_unary, sparsity_binary = get_expressions(nsr)\n",
    "print('Unroll network into symbolic expressions (input sparsity = {0:.3f}; model sparsity = {1:.3f}; unary sparsity = {2:.3f}; binary sparsity = {2:.3f})\\n--------------'.format(sparsity_input, sparsity_model, sparsity_unary, sparsity_binary))\n",
    "print('Mean complexity = {0:.1f}\\n--------------'.format(np.mean(complexity)))\n",
    "for i in range(expressions_masked.shape[1]):\n",
    "    print('expr_{0} (complexity = {1}):\\n\\n{2}\\n-------------------------------------'.format(i,complexity[i],expressions_masked[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d00c6-1966-4f38-bb31-d5f7eceda98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
