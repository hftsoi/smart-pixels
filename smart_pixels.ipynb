{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a3bd06-f806-4ebc-9cd5-3cd9c6c8c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import sympy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, r2_score\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.initializers import Zeros, RandomNormal\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "import qkeras\n",
    "from qkeras import *\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5984d-9609-4bac-b3e4-db593dfcf583",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train.npy', mmap_mode='r')\n",
    "X_val = np.load('X_val.npy', mmap_mode='r')\n",
    "X_test = np.load('X_test.npy', mmap_mode='r')\n",
    "Y_train = np.load('Y_train.npy', mmap_mode='r')\n",
    "Y_val = np.load('Y_val.npy', mmap_mode='r')\n",
    "Y_test = np.load('Y_test.npy', mmap_mode='r')\n",
    "pt_truth_test = np.load('pt_truth_test.npy', mmap_mode='r')\n",
    "\n",
    "print('X_train shape: ' + str(X_train.shape))\n",
    "print('X_val   shape: ' + str(X_val.shape))\n",
    "print('X_test  shape: ' + str(X_test.shape))\n",
    "print('Y_train shape: ' + str(Y_train.shape))\n",
    "print('Y_val   shape: ' + str(Y_val.shape))\n",
    "print('Y_test  shape: ' + str(Y_test.shape))\n",
    "print('pt_truth_test  shape: ' + str(pt_truth_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad2e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:,:2]\n",
    "X_val = X_val[:,:2]\n",
    "X_test = X_test[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be35e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c84c662-c642-48e0-a266-9c040281e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = (Y_train[:, 2] == 1).astype(float).reshape(-1, 1)\n",
    "Y_val = (Y_val[:, 2] == 1).astype(float).reshape(-1, 1)\n",
    "Y_test = (Y_test[:, 2] == 1).astype(float).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3dcd33-bf93-42f2-91fd-8d56797fb55c",
   "metadata": {},
   "source": [
    "## nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa29564-b382-4443-9d44-b530c0c604c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = quantized_bits(16, 6, alpha=1)\n",
    "quantized_relu = 'quantized_relu(16, 6)'\n",
    "\n",
    "x_input = keras.Input(shape=(2,), name='input')\n",
    "\n",
    "x = QDense(128, use_bias=True, name='dense1', kernel_quantizer=quantizer, bias_quantizer=quantizer)(x_input)\n",
    "x = QActivation(quantized_relu, name='relu1')(x)\n",
    "    \n",
    "x = QDense(3, use_bias=True, name='dense2', kernel_quantizer=quantizer, bias_quantizer=quantizer)(x)\n",
    "x = layers.Softmax(name='softmax')(x)\n",
    "#x = layers.Activation('sigmoid')(x)\n",
    "\n",
    "model = keras.Model(x_input, x, name='model')\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0005),\n",
    "              loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "#model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),\n",
    "#              loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56450055-5f05-407b-b024-a3ee3bbd560f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    validation_data = (X_val, Y_val),\n",
    "                    epochs=60, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bec536-8ac6-4df9-a34d-d69a8178647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "axes = plt.subplot(2, 2, 1)\n",
    "axes.plot(history.history['loss'], label = 'train loss')\n",
    "axes.plot(history.history['val_loss'], label = 'val loss')\n",
    "axes.legend(loc = \"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770756f9-011f-40ab-a5cf-092bfec2c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_nn_pred = model.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(Y_test, axis=1), np.argmax(Y_nn_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a852df2-4b05-4d40-addd-2092ab3eac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_test, y_pred, labels):\n",
    "    for x, label in enumerate(labels):        \n",
    "        fpr, tpr, _ = roc_curve(y_test[:, x], y_pred[:, x])\n",
    "        plt.plot(fpr, tpr, label='{0}, {1:.1f}'.format(label, auc(fpr, tpr)*100.), linestyle='-')\n",
    "    #plt.semilogy()\n",
    "    #plt.semilogx()\n",
    "    plt.ylabel(\"Signal Efficiency\")\n",
    "    plt.xlabel(\"Background Efficiency\")\n",
    "    #plt.ylim(0.00001, 1)\n",
    "    #plt.xlim(0.00001, 1)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best', fontsize=10)  \n",
    "    \n",
    "plt.figure(figsize=(4, 4))\n",
    "plot_roc(Y_test, Y_nn_pred, ['Low pT (pos)','Low pT (neg)','High pT'])\n",
    "#plot_roc(Y_test, Y_nn_pred, [' '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3c1bd-0409-406d-90ad-1205e0526eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bins = np.arange(-4.5, 4.5, 0.01)\n",
    "bin_centers = 0.5 * (custom_bins[:-1] + custom_bins[1:])\n",
    "\n",
    "indices_001_nn = np.argmax(Y_nn_pred, axis=1) == 2\n",
    "\n",
    "total_counts_nn, _ = np.histogram(pt_truth_test, bins=custom_bins)\n",
    "class_001_counts_nn, _ = np.histogram(pt_truth_test[indices_001_nn], bins=custom_bins)\n",
    "\n",
    "proportions_nn = class_001_counts_nn / total_counts_nn\n",
    "proportions_nn = np.nan_to_num(proportions_nn)\n",
    "\n",
    "plt.scatter(bin_centers, proportions_nn, marker='.', label='nn', alpha=0.5, color='blue')\n",
    "plt.axvline(x=0.2, color='grey', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=-0.2, color='grey', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel('True pT [GeV]')\n",
    "plt.ylabel('Fraction')\n",
    "plt.title('Fraction of clusters selected as having |pT| > 0.2 GeV')\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "#plt.yscale('log')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8892e-0ef5-480a-87df-5c357250ba15",
   "metadata": {},
   "source": [
    "## symbolnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a597f43-97e1-41a0-a7b2-2bd610f94b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_operation(tf_or_sympy, operator, x, y=None):\n",
    "    # unary operators\n",
    "    if operator == 'identity':\n",
    "        output = x\n",
    "    elif operator == 'sin':\n",
    "        output = tf.sin(x) if tf_or_sympy == 'tf' else sympy.sin(x)\n",
    "    elif operator == 'cos':\n",
    "        output = tf.cos(x) if tf_or_sympy == 'tf' else sympy.cos(x)\n",
    "    elif operator == 'exp':\n",
    "        output = tf.exp(x) if tf_or_sympy == 'tf' else sympy.exp(x)\n",
    "    elif operator == 'gauss':\n",
    "        output = tf.exp(-x**2) if tf_or_sympy == 'tf' else sympy.exp(-x**2)\n",
    "    elif operator == 'sinh':\n",
    "        output = tf.sinh(x) if tf_or_sympy == 'tf' else sympy.sinh(x)\n",
    "    elif operator == 'cosh':\n",
    "        output = tf.cosh(x) if tf_or_sympy == 'tf' else sympy.cosh(x)\n",
    "    elif operator == 'tanh':\n",
    "        output = tf.tanh(x) if tf_or_sympy == 'tf' else sympy.tanh(x)\n",
    "    elif operator == 'square':\n",
    "        output = x**2 if tf_or_sympy == 'tf' else x**2\n",
    "    elif operator == 'cube':\n",
    "        output = x**3 if tf_or_sympy == 'tf' else x**3\n",
    "    elif operator == 'log':\n",
    "        output = tf.math.log(0.001 + tf.abs(x)) if tf_or_sympy == 'tf' else sympy.log(0.001 + sympy.Abs(x))\n",
    "        #output = tf.math.log(tf.abs(x)) if tf_or_sympy == 'tf' else sympy.log(sympy.Abs(x))\n",
    "    # binary operators\n",
    "    elif operator == '+':\n",
    "        output = x + y\n",
    "    elif operator == '*':\n",
    "        output = x * y\n",
    "    elif operator == 'pow':\n",
    "        output = x ** y\n",
    "    elif operator == '/':\n",
    "        output = x / (0.001 + tf.abs(y)) if tf_or_sympy == 'tf' else x / (0.001 + sympy.Abs(y))\n",
    "        #output = x / y\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8acb38b-7d78-448d-b5ce-c8ca999c6446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step function for masking in the forward pass\n",
    "# custom grad is an estimator of derivative of the step function to avoid vanishing gradient\n",
    "@tf.custom_gradient\n",
    "def step_func(x):\n",
    "    func = tf.where(x > 0., 1., 0.)\n",
    "    def grad(upstream):\n",
    "        a = 5.\n",
    "        return upstream * a * tf.exp(-a*x) / (1 + tf.exp(-a*x))**2\n",
    "    return func, grad\n",
    "\n",
    "# dynamic pruning of input features\n",
    "# define one auxiliary weight and one untrainable threshold per input feature\n",
    "class Input_sparsity(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # auxiliary weight is untrainable and fixed at 1\n",
    "        self.aux_w = self.add_weight(name='weight',\n",
    "                                     shape=(input_shape[-1],),\n",
    "                                     initializer='ones',\n",
    "                                     trainable=False)\n",
    "        # threshold is trainable, initialized at 0, and bounded in [0,1]\n",
    "        self.aux_w_t = self.add_weight(name='threshold',\n",
    "                                     shape=(input_shape[-1],),\n",
    "                                     initializer='zeros',\n",
    "                                     constraint=lambda x: tf.clip_by_value(x, 0., 0.5),\n",
    "                                     trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_masks = step_func(self.aux_w - self.aux_w_t)\n",
    "        return tf.multiply(inputs, input_masks)\n",
    "\n",
    "# definition of symbolic layer (usual weights and biases, plus unary/binary operators as activations)\n",
    "# define trainable thresholds for weights/biases/unary/binary\n",
    "class Symbolic_Layer(Layer):\n",
    "    def __init__(self, operators, num_operators):\n",
    "        super().__init__()\n",
    "        self.operators = operators\n",
    "        self.num_operators = num_operators\n",
    "        self.units = self.num_operators[0] + 2*self.num_operators[1]\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # usual model weight w\n",
    "        self.w = self.add_weight(name='weight',\n",
    "                                 shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        # usual bias b\n",
    "        self.b = self.add_weight(name='bias',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        # auxiliary weight for unary operator, to be used for operator pruning\n",
    "        # untrainable and fixed at 1\n",
    "        self.aux_unary = self.add_weight(name='unary',\n",
    "                                         shape=(self.num_operators[0],),\n",
    "                                         initializer='ones',\n",
    "                                         trainable=False)\n",
    "        # auxiliary weight for binary operator, to be used for operator pruning\n",
    "        # untrainable and fixed at 1\n",
    "        if self.num_operators[1] > 0:\n",
    "            self.aux_binary = self.add_weight(name='binary',\n",
    "                                              shape=(self.num_operators[1],),\n",
    "                                              initializer='ones',\n",
    "                                              trainable=False)\n",
    "        # threshold for model weight\n",
    "        # trainable, initialized at 0, unbounded since model weight is unbounded\n",
    "        self.aux_w_t = self.add_weight(name='weight_threshold',\n",
    "                                       shape=(input_shape[-1], self.units),\n",
    "                                       initializer='zeros',\n",
    "                                       constraint=lambda x: tf.abs(x),\n",
    "                                       trainable=True)\n",
    "        # threshold for bias term\n",
    "        # trainable, initialized at 0, unbounded since model weight is unbounded\n",
    "        self.aux_b_t = self.add_weight(name='bias_threshold',\n",
    "                                       shape=(self.units,),\n",
    "                                       initializer='zeros',\n",
    "                                       constraint=lambda x: tf.abs(x),\n",
    "                                       trainable=True)\n",
    "        # threshold for unary operator\n",
    "        # trainable, initialized at 0, bounded in [0,1]\n",
    "        self.aux_unary_t = self.add_weight(name='unary_threshold',\n",
    "                                          shape=(self.num_operators[0],),\n",
    "                                          initializer='zeros',\n",
    "                                          constraint=lambda x: tf.clip_by_value(x, 0., 1.),\n",
    "                                          trainable=True)\n",
    "        # threshold for binary operator\n",
    "        # trainable, initialized at 0, bounded in [0,1]\n",
    "        if self.num_operators[1] > 0:\n",
    "            self.aux_binary_t = self.add_weight(name='binary_threshold',\n",
    "                                                shape=(self.num_operators[1],),\n",
    "                                                initializer='zeros',\n",
    "                                                constraint=lambda x: tf.clip_by_value(x, 0., 1.),\n",
    "                                                trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # linear transformation in the forward pass\n",
    "        # weights and biases are replaced by the step_functioned version\n",
    "        # so weight is masked whenever its threshold is higher\n",
    "        w_masks = step_func(tf.abs(self.w) - self.aux_w_t)\n",
    "        b_masks = step_func(tf.abs(self.b) - self.aux_b_t)\n",
    "        linear_output = tf.matmul(inputs, tf.multiply(self.w, w_masks)) + tf.multiply(self.b, b_masks)\n",
    "        \n",
    "        # activation by unary/binary operator\n",
    "        symbolic_output = []\n",
    "        \n",
    "        # loop over number of unary operators in a symbolic layer\n",
    "        for i in range(self.num_operators[0]):\n",
    "            # an unary operator is \"pruned\" (becomes identity map) whenever the threshold is higher than its auxiliary weight\n",
    "            unary_mask = step_func(self.aux_unary - self.aux_unary_t)[i]\n",
    "            idx = np.mod(i, len(self.operators[0]))\n",
    "            unary_operation = (unary_mask * math_operation('tf',self.operators[0][idx],linear_output[:, i:i+1]) +\n",
    "                               (1.0 - unary_mask) * math_operation('tf','identity',linear_output[:, i:i+1]))\n",
    "            symbolic_output.append(unary_operation)\n",
    "            \n",
    "        # loop over number of binary operators in a symbolic layer\n",
    "        for i in range(self.num_operators[0], self.num_operators[0] + 2*self.num_operators[1], 2):\n",
    "            # a binary operator is \"pruned\" (becomes addition) whenever the threshold is higher than its auxiliary weight\n",
    "            j = int((i - self.num_operators[0])/2)\n",
    "            binary_mask = step_func(self.aux_binary - self.aux_binary_t)[j]\n",
    "            idx = np.mod(j, len(self.operators[1]))\n",
    "            binary_operation = (binary_mask * math_operation('tf',self.operators[1][idx],linear_output[:, i:i+1],linear_output[:, i+1:i+2]) +\n",
    "                                (1.0 - binary_mask) * math_operation('tf','+',linear_output[:, i:i+1],linear_output[:, i+1:i+2]))\n",
    "            symbolic_output.append(binary_operation)\n",
    "        \n",
    "        symbolic_output = tf.concat(symbolic_output, axis=1)\n",
    "        return symbolic_output\n",
    "    \n",
    "def create_model(model_dim, operators, num_operators):\n",
    "    input_dim, num_hidden_layers, output_dim = model_dim\n",
    "    layers = []\n",
    "    \n",
    "    # input layer\n",
    "    layers.append(Input(shape=(input_dim,)))\n",
    "    layers.append(Input_sparsity()(layers[-1]))\n",
    "    \n",
    "    # hidden symbolic layers\n",
    "    for i in range(num_hidden_layers):\n",
    "        layers.append(Symbolic_Layer(operators=operators[i],\n",
    "                                     num_operators=num_operators[i])(layers[-1]))\n",
    "    \n",
    "    # output layer\n",
    "    layers.append(Symbolic_Layer(operators=[['identity'], [None]],\n",
    "                                 num_operators=[output_dim, 0])(layers[-1]))\n",
    "    \n",
    "    model = keras.Model(inputs=layers[0], outputs=layers[-1], name='model')\n",
    "    return model, model_dim, operators, num_operators\n",
    "\n",
    "input_dim  = X_train.shape[1]\n",
    "num_hidden = 1\n",
    "output_dim = Y_train.shape[1]\n",
    "model_dim = [input_dim, num_hidden, output_dim]\n",
    "\n",
    "# operator choices per hidden symbolic layer\n",
    "operators = [\n",
    "    [['sin','exp','tanh'], ['*']], # 1st symbolic layer [[unary], [binary]]\n",
    "    #[['sin','exp','tanh'], ['*']], # 2nd ...\n",
    "            ]\n",
    "\n",
    "# number of unary and binary operators per hidden symbolic layer\n",
    "num_operators = [\n",
    "    [50, 50], # 1st symbolic layer [num_unary, num_binary]\n",
    "    #[20, 4], # 2nd ...\n",
    "]\n",
    "\n",
    "model = create_model(model_dim=model_dim,\n",
    "                     operators=operators,\n",
    "                     num_operators=num_operators)\n",
    "model[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4221fe0e-a49f-41c7-b73a-c960855f4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralSR(keras.Model):\n",
    "    def __init__(self, model, alpha_sparsity_input, alpha_sparsity_model, alpha_sparsity_unary, alpha_sparsity_binary):\n",
    "        super().__init__()\n",
    "        self.model, self.model_dim, self.operators, self.num_operators = model\n",
    "        self.alpha_sparsity_input = alpha_sparsity_input\n",
    "        self.alpha_sparsity_model = alpha_sparsity_model\n",
    "        self.alpha_sparsity_unary = alpha_sparsity_unary\n",
    "        self.alpha_sparsity_binary = alpha_sparsity_binary\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.regression_loss_tracker = keras.metrics.Mean(name=\"regression_loss\")\n",
    "        self.threshold_input_reg_loss_tracker = keras.metrics.Mean(name=\"threshold_input_reg_loss\")\n",
    "        self.threshold_input_mean_tracker = keras.metrics.Mean(name=\"threshold_input_mean\")\n",
    "        self.threshold_model_reg_loss_tracker = keras.metrics.Mean(name=\"threshold_model_reg_loss\")\n",
    "        self.threshold_model_mean_tracker = keras.metrics.Mean(name=\"threshold_model_mean\")\n",
    "        self.threshold_unary_reg_loss_tracker = keras.metrics.Mean(name=\"threshold_unary_reg_loss\")\n",
    "        self.threshold_unary_mean_tracker = keras.metrics.Mean(name=\"threshold_unary_mean\")\n",
    "        self.threshold_binary_reg_loss_tracker = keras.metrics.Mean(name=\"threshold_binary_reg_loss\")\n",
    "        self.threshold_binary_mean_tracker = keras.metrics.Mean(name=\"threshold_binary_mean\")\n",
    "        self.weight_input_mean_tracker = keras.metrics.Mean(name=\"weight_input_mean\")\n",
    "        self.weight_model_mean_tracker = keras.metrics.Mean(name=\"weight_model_mean\")\n",
    "        self.weight_unary_mean_tracker = keras.metrics.Mean(name=\"weight_unary_mean\")\n",
    "        self.weight_binary_mean_tracker = keras.metrics.Mean(name=\"weight_binary_mean\")\n",
    "        self.sparsity_input_tracker = keras.metrics.Mean(name=\"sparsity_input\")\n",
    "        self.sparsity_model_tracker = keras.metrics.Mean(name=\"sparsity_model\")\n",
    "        self.sparsity_unary_tracker = keras.metrics.Mean(name=\"sparsity_unary\")\n",
    "        self.sparsity_binary_tracker = keras.metrics.Mean(name=\"sparsity_binary\")\n",
    "        self.accuracy_tracker = keras.metrics.Accuracy(name=\"accuracy\")\n",
    "    \n",
    "    def get_hyperparameters(self):\n",
    "        return self.model_dim, self.operators, self.num_operators\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.regression_loss_tracker,\n",
    "            self.threshold_input_reg_loss_tracker,\n",
    "            self.threshold_input_mean_tracker,\n",
    "            self.threshold_model_reg_loss_tracker,\n",
    "            self.threshold_model_mean_tracker,\n",
    "            self.threshold_unary_reg_loss_tracker,\n",
    "            self.threshold_unary_mean_tracker,\n",
    "            self.threshold_binary_reg_loss_tracker,\n",
    "            self.threshold_binary_mean_tracker,\n",
    "            self.weight_input_mean_tracker,\n",
    "            self.weight_model_mean_tracker,\n",
    "            self.weight_unary_mean_tracker,\n",
    "            self.weight_binary_mean_tracker,\n",
    "            self.sparsity_input_tracker,\n",
    "            self.sparsity_model_tracker,\n",
    "            self.sparsity_unary_tracker,\n",
    "            self.sparsity_binary_tracker,\n",
    "            self.accuracy_tracker\n",
    "        ]\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.model(x)\n",
    "            \n",
    "            # base training loss (MSE)\n",
    "            regression_loss = tf.reduce_mean(tf.reduce_sum(tf.cast((y-y_pred)**2,dtype=tf.float64), axis=1))\n",
    "            \n",
    "            # the followings calculate sparsity levels at train steps\n",
    "            # since total loss = MSE + sparsity regularization terms\n",
    "            # where sparsity regularization terms depend on sparsity levels\n",
    "            \n",
    "            # call auxiliary weights and thresholds for inputs\n",
    "            w_input = self.non_trainable_weights[0]\n",
    "            w_t_input = self.trainable_weights[0]\n",
    "            num_input_masks = tf.reduce_sum(tf.cast(tf.where(w_input - w_t_input > 0., 0., 1.), dtype=tf.float64))\n",
    "            num_input_weights = tf.size(w_input, out_type=tf.float64)\n",
    "            sparsity_input = num_input_masks/num_input_weights\n",
    "            weight_input_mean = tf.reduce_sum(tf.cast(tf.abs(w_input),dtype=tf.float64))/num_input_weights\n",
    "            \n",
    "            # calculate input sparsity\n",
    "            t_input_sum = tf.reduce_sum(tf.cast(w_t_input,dtype=tf.float64))\n",
    "            t_input_dim = num_input_weights\n",
    "            threshold_input_mean = t_input_sum/t_input_dim\n",
    "            \n",
    "            # call model weights and thresholds for all hidden symbolic layers\n",
    "            # then calculate model weight (weight+bias) sparsity\n",
    "            num_model_masks = 0.\n",
    "            num_model_weights = 0.\n",
    "            weight_model_mean = 0.\n",
    "            t_model_sum = 0.\n",
    "            t_model_dim = 0.\n",
    "            sum_exp_t = 0.\n",
    "            for i in range(self.model_dim[1]+1):\n",
    "                w_model = self.trainable_weights[1+6*i]\n",
    "                b_model = self.trainable_weights[1+6*i+1]\n",
    "                w_t_model = self.trainable_weights[1+6*i+2]\n",
    "                b_t_model = self.trainable_weights[1+6*i+3]\n",
    "                sum_exp_t += tf.reduce_sum(tf.exp(-tf.cast(w_t_model,dtype=tf.float64)))\n",
    "                sum_exp_t += tf.reduce_sum(tf.exp(-tf.cast(b_t_model,dtype=tf.float64)))\n",
    "                num_model_masks += tf.reduce_sum(tf.cast(tf.where(-w_t_model + tf.abs(w_model) > 0., 0., 1.),dtype=tf.float64))\n",
    "                num_model_masks += tf.reduce_sum(tf.cast(tf.where(-b_t_model + tf.abs(b_model) > 0., 0., 1.),dtype=tf.float64))\n",
    "                num_model_weights += tf.size(w_model, out_type=tf.float64)\n",
    "                num_model_weights += tf.size(b_model, out_type=tf.float64)\n",
    "                weight_model_mean += tf.reduce_sum(tf.cast(tf.abs(w_model),dtype=tf.float64)) + tf.reduce_sum(tf.cast(tf.abs(b_model),dtype=tf.float64))\n",
    "                t_model_dim += tf.size(w_t_model, out_type=tf.float64) + tf.size(b_t_model, out_type=tf.float64)\n",
    "                t_model_sum += tf.reduce_sum(tf.cast(w_t_model,dtype=tf.float64)) + tf.reduce_sum(tf.cast(b_t_model,dtype=tf.float64))\n",
    "            sparsity_model = num_model_masks/num_model_weights\n",
    "            weight_model_mean = weight_model_mean/num_model_weights\n",
    "\n",
    "            threshold_model_mean = t_model_sum/t_model_dim\n",
    "            \n",
    "            # call auxiliary weights and thresholds for unary and binary operators\n",
    "            num_unary_masks = 0.\n",
    "            num_unary_weights = 0.\n",
    "            weight_unary_mean = 0.\n",
    "            num_binary_masks = 0.\n",
    "            num_binary_weights = 0.\n",
    "            weight_binary_mean = 0.\n",
    "            t_u_sum = 0.\n",
    "            t_u_dim = 0.\n",
    "            t_b_sum = 0.\n",
    "            t_b_dim = 0.\n",
    "            for i in range(self.model_dim[1]):\n",
    "                u = self.non_trainable_weights[2*i+1]\n",
    "                u_t = self.trainable_weights[1+6*i+4]\n",
    "                num_unary_masks += tf.reduce_sum(tf.cast(tf.where(u - u_t > 0., 0., 1.),dtype=tf.float64))\n",
    "                num_unary_weights += tf.size(u, out_type=tf.float64)\n",
    "                weight_unary_mean += tf.reduce_sum(tf.cast(u,dtype=tf.float64))\n",
    "                t_u_dim += tf.size(u_t, out_type=tf.float64)\n",
    "                t_u_sum += tf.reduce_sum(tf.cast(u_t,dtype=tf.float64))\n",
    "                \n",
    "                b = self.non_trainable_weights[2*i+2]\n",
    "                b_t = self.trainable_weights[1+6*i+5]\n",
    "                num_binary_masks += tf.reduce_sum(tf.cast(tf.where(b - b_t > 0., 0., 1.),dtype=tf.float64))\n",
    "                num_binary_weights += tf.size(b, out_type=tf.float64)\n",
    "                weight_binary_mean += tf.reduce_sum(tf.cast(b,dtype=tf.float64))\n",
    "                t_b_dim += tf.size(b_t, out_type=tf.float64)\n",
    "                t_b_sum += tf.reduce_sum(tf.cast(b_t,dtype=tf.float64))\n",
    "                    \n",
    "            # calculate sparsity levels for unary and binary operators\n",
    "            sparsity_unary = num_unary_masks/num_unary_weights\n",
    "            sparsity_binary = num_binary_masks/num_binary_weights\n",
    "            weight_unary_mean = weight_unary_mean/num_unary_weights\n",
    "            weight_binary_mean = weight_binary_mean/num_binary_weights\n",
    "            \n",
    "            threshold_unary_mean = t_u_sum/t_u_dim\n",
    "            threshold_binary_mean = t_b_sum/t_b_dim\n",
    "            \n",
    "            # sparsity regularization terms\n",
    "            threshold_input_reg_loss = regression_loss*tf.exp(-threshold_input_mean)\n",
    "            threshold_model_reg_loss = regression_loss*sum_exp_t/num_model_weights\n",
    "            threshold_unary_reg_loss = regression_loss*tf.exp(-threshold_unary_mean)\n",
    "            threshold_binary_reg_loss = regression_loss*tf.exp(-threshold_binary_mean)\n",
    "            # additional decay factor\n",
    "            def reg(s, s_t, d):\n",
    "                return tf.exp(-(s_t/(s_t-tf.minimum(s, s_t)))**d+1.)\n",
    "            threshold_input_reg_loss *= reg(sparsity_input, self.alpha_sparsity_input, 0.01)\n",
    "            threshold_model_reg_loss *= reg(sparsity_model, self.alpha_sparsity_model, 0.01)\n",
    "            threshold_unary_reg_loss *= reg(sparsity_unary, self.alpha_sparsity_unary, 0.01)\n",
    "            threshold_binary_reg_loss *= reg(sparsity_binary, self.alpha_sparsity_binary, 0.01)\n",
    "            \n",
    "            # total loss\n",
    "            total_loss = regression_loss + (threshold_model_reg_loss + \n",
    "                                            threshold_input_reg_loss + \n",
    "                                            threshold_unary_reg_loss +\n",
    "                                            threshold_binary_reg_loss)\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "                \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.regression_loss_tracker.update_state(regression_loss)\n",
    "        self.threshold_input_reg_loss_tracker.update_state(threshold_input_reg_loss)\n",
    "        self.threshold_input_mean_tracker.update_state(threshold_input_mean)\n",
    "        self.threshold_model_reg_loss_tracker.update_state(threshold_model_reg_loss)\n",
    "        self.threshold_model_mean_tracker.update_state(threshold_model_mean)\n",
    "        self.threshold_unary_reg_loss_tracker.update_state(threshold_unary_reg_loss)\n",
    "        self.threshold_unary_mean_tracker.update_state(threshold_unary_mean)\n",
    "        self.threshold_binary_reg_loss_tracker.update_state(threshold_binary_reg_loss)\n",
    "        self.threshold_binary_mean_tracker.update_state(threshold_binary_mean)\n",
    "        self.weight_model_mean_tracker.update_state(weight_model_mean)\n",
    "        self.weight_input_mean_tracker.update_state(weight_input_mean)\n",
    "        self.weight_unary_mean_tracker.update_state(weight_unary_mean)\n",
    "        self.weight_binary_mean_tracker.update_state(weight_binary_mean)\n",
    "        self.accuracy_tracker.update_state(tf.argmax(y, axis=1), tf.argmax(y_pred, axis=1))\n",
    "        self.sparsity_input_tracker.update_state(sparsity_input)\n",
    "        self.sparsity_model_tracker.update_state(sparsity_model)\n",
    "        self.sparsity_unary_tracker.update_state(sparsity_unary)\n",
    "        self.sparsity_binary_tracker.update_state(sparsity_binary)\n",
    "        return {\n",
    "            'loss': self.total_loss_tracker.result(),\n",
    "            'regression_loss': self.regression_loss_tracker.result(),\n",
    "            'threshold_input_reg_loss': self.threshold_input_reg_loss_tracker.result(),\n",
    "            'threshold_input_mean': self.threshold_input_mean_tracker.result(),\n",
    "            'threshold_model_reg_loss': self.threshold_model_reg_loss_tracker.result(),\n",
    "            'threshold_model_mean': self.threshold_model_mean_tracker.result(),\n",
    "            'threshold_unary_reg_loss': self.threshold_unary_reg_loss_tracker.result(),\n",
    "            'threshold_unary_mean': self.threshold_unary_mean_tracker.result(),\n",
    "            'threshold_binary_reg_loss': self.threshold_binary_reg_loss_tracker.result(),\n",
    "            'threshold_binary_mean': self.threshold_binary_mean_tracker.result(),\n",
    "            'weight_model_mean': self.weight_model_mean_tracker.result(),\n",
    "            #'weight_input_mean': self.weight_input_mean_tracker.result(),\n",
    "            #'weight_unary_mean': self.weight_unary_mean_tracker.result(),\n",
    "            #'weight_binary_mean': self.weight_binary_mean_tracker.result(),\n",
    "            'sparsity_input': self.sparsity_input_tracker.result(),\n",
    "            'sparsity_model': self.sparsity_model_tracker.result(),\n",
    "            'sparsity_unary': self.sparsity_unary_tracker.result(),\n",
    "            'sparsity_binary': self.sparsity_binary_tracker.result(),\n",
    "            'accuracy': self.accuracy_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202acc3-8f19-4c5f-a389-33763a2eb2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsr = neuralSR(model,\n",
    "               # set target sparsity level per pruning type\n",
    "               alpha_sparsity_input=0.1,\n",
    "               alpha_sparsity_model=0.5,\n",
    "               alpha_sparsity_unary=0.2,\n",
    "               alpha_sparsity_binary=0.2)\n",
    "nsr.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601f5ec-4b9d-4c21-89cd-3293d50ffdd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = nsr.fit(X_train, Y_train, epochs=15, batch_size=128)\n",
    "history = dict()\n",
    "for key in h.history.keys():\n",
    "    values = []\n",
    "    values += h.history[key]\n",
    "    history[key] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2fa48f-f848-4645-811b-6d474dcdcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_axis_title=10\n",
    "size_axis_label=8\n",
    "size_legend=7\n",
    "plt.figure(figsize = (7,7))\n",
    "axes = plt.subplot(2,2,1)\n",
    "axes.set_ylim([-0.05,1.1])\n",
    "plt.xticks(fontsize = size_axis_label) \n",
    "plt.yticks(fontsize = size_axis_label) \n",
    "axes.plot(history['accuracy'], label='Accuracy', linestyle='solid', c='r')\n",
    "axes.plot(history['sparsity_model'], label='Sparsity (weight)', linestyle='solid', c='g')\n",
    "axes.plot(history['sparsity_input'], label='Sparsity (input)', linestyle='solid', c='b')\n",
    "axes.plot(history['sparsity_unary'], label='Sparsity (unary)', linestyle='solid', c='orange')\n",
    "axes.plot(history['sparsity_binary'], label='Sparsity (binary)', linestyle='solid', c='brown')\n",
    "axes.set_xlabel('Epoch', size=size_axis_title, loc='right')\n",
    "axes.set_ylabel('Metric', size=size_axis_title, loc='top')\n",
    "axes.legend(loc = 'best', frameon = False, fontsize = size_legend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc4fc3c-253b-4693-bab6-3e1709831147",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sr_pred = nsr.model.predict(X_test)\n",
    "print(\"Accuracy = {}\".format(accuracy_score(np.argmax(Y_test, axis=1), np.argmax(Y_sr_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24715d3f-faa5-4b07-8312-95582b5e8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_test, y_pred, labels):\n",
    "    for x, label in enumerate(labels):        \n",
    "        fpr, tpr, _ = roc_curve(y_test[:, x], y_pred[:, x])\n",
    "        plt.plot(fpr, tpr, label='{0}, {1:.1f}'.format(label, auc(fpr, tpr)*100.), linestyle='-')\n",
    "    #plt.semilogy()\n",
    "    #plt.semilogx()\n",
    "    plt.ylabel(\"Signal Efficiency\")\n",
    "    plt.xlabel(\"Background Efficiency\")\n",
    "    #plt.ylim(0.00001, 1)\n",
    "    #plt.xlim(0.00001, 1)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best', fontsize=10)  \n",
    "    \n",
    "plt.figure(figsize=(4, 4))\n",
    "plot_roc(Y_test, Y_sr_pred, ['Low pT (pos)','Low pT (neg)','High pT'])\n",
    "#plot_roc(Y_test, Y_sr_pred, [' '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382de1de-783e-47c3-8e4e-ed39556a8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bins = np.arange(-4.5, 4.5, 0.01)\n",
    "bin_centers = 0.5 * (custom_bins[:-1] + custom_bins[1:])\n",
    "\n",
    "indices_001_sr = np.argmax(Y_sr_pred, axis=1) == 2\n",
    "\n",
    "total_counts_sr, _ = np.histogram(pt_truth_test, bins=custom_bins)\n",
    "class_001_counts_sr, _ = np.histogram(pt_truth_test[indices_001_sr], bins=custom_bins)\n",
    "\n",
    "proportions_sr = class_001_counts_sr / total_counts_sr\n",
    "proportions_sr = np.nan_to_num(proportions_sr)\n",
    "\n",
    "plt.scatter(bin_centers, proportions_nn, marker='.', label='nn', alpha=0.5, color='blue')\n",
    "plt.scatter(bin_centers, proportions_sr, marker='.', label='sr', alpha=0.5, color='red')\n",
    "plt.axvline(x=0.2, color='grey', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=-0.2, color='grey', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel('True pT [GeV]')\n",
    "plt.ylabel('Fraction')\n",
    "plt.title('Fraction of clusters selected as having |pT| > 0.2 GeV')\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "#plt.yscale('log')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570d705-5cb4-4f72-9620-056c8a8e0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set significant digits for expression display\n",
    "significant_digits = 2\n",
    "\n",
    "# use sympy to expand the trained model\n",
    "def get_expressions(neuralSR):\n",
    "    model_dim, operators, num_operators = neuralSR.get_hyperparameters()\n",
    "    input_dim, num_hidden_layers, output_dim = model_dim\n",
    "    \n",
    "    x=[]\n",
    "    for i in range(input_dim):\n",
    "        x.append(sympy.Symbol('x{}'.format(i)))\n",
    "    x_masked = sympy.Matrix([x])\n",
    "    \n",
    "    w_input = neuralSR.model.layers[1].get_weights()[1]\n",
    "    t_input = neuralSR.model.layers[1].get_weights()[0]\n",
    "    num_input_masks = tf.reduce_sum(tf.cast(tf.where(w_input-t_input>0., 0., 1.),dtype=tf.float64))\n",
    "    num_input_weights = tf.size(w_input, out_type=tf.float64)\n",
    "    \n",
    "    sparsity_input = num_input_masks/num_input_weights\n",
    "    \n",
    "    w_input_masked = sympy.Matrix(tf.where(w_input-t_input>0., w_input, 0.))\n",
    "    x_print = np.multiply(sympy.Transpose(w_input_masked), x_masked)\n",
    "    x_masked = np.multiply(sympy.Transpose(w_input_masked), x_masked)\n",
    "\n",
    "    print('Remaining Inputs after pruning: {}\\n'.format(str(x_masked).replace('1.0*','')))\n",
    "    \n",
    "    num_masks = 0.\n",
    "    num_weights = 0.\n",
    "    \n",
    "    num_unary_masks = 0.\n",
    "    num_binary_masks = 0.\n",
    "    num_unary = 0.\n",
    "    num_binary = 0.\n",
    "    \n",
    "    for i in range(num_hidden_layers+1):\n",
    "        w = neuralSR.model.layers[i+2].get_weights()[0]\n",
    "        b = neuralSR.model.layers[i+2].get_weights()[1]\n",
    "        w_t = neuralSR.model.layers[i+2].get_weights()[2]\n",
    "        b_t = neuralSR.model.layers[i+2].get_weights()[3]\n",
    "        \n",
    "        num_masks += tf.reduce_sum(tf.cast(tf.where(tf.abs(w) - w_t > 0., 0., 1.),dtype=tf.float64))\n",
    "        num_masks += tf.reduce_sum(tf.cast(tf.where(tf.abs(b) - b_t > 0., 0., 1.),dtype=tf.float64))\n",
    "        num_weights += tf.size(w, out_type=tf.float64)\n",
    "        num_weights += tf.size(b, out_type=tf.float64)\n",
    "        \n",
    "        w_masked = sympy.Matrix(tf.where(tf.abs(w) - w_t > 0., w, 0.))\n",
    "        b_masked = sympy.Transpose(sympy.Matrix(tf.where(tf.abs(b) - b_t > 0., b, 0.)))\n",
    "        \n",
    "        x_masked = (x_masked * w_masked + b_masked).evalf(significant_digits)\n",
    "        if i < num_hidden_layers:\n",
    "            unary = neuralSR.model.layers[i+2].get_weights()[6]\n",
    "            unary_t = neuralSR.model.layers[i+2].get_weights()[4]\n",
    "            binary = neuralSR.model.layers[i+2].get_weights()[7]\n",
    "            binary_t = neuralSR.model.layers[i+2].get_weights()[5]\n",
    "            num_unary_masks += tf.reduce_sum(tf.cast(tf.where(unary - unary_t > 0., 0., 1.),dtype=tf.float64))\n",
    "            num_binary_masks += tf.reduce_sum(tf.cast(tf.where(binary - binary_t > 0., 0., 1.),dtype=tf.float64))\n",
    "            num_unary += tf.size(unary, out_type=tf.float64)\n",
    "            num_binary += tf.size(binary, out_type=tf.float64)\n",
    "        elif i == num_hidden_layers:\n",
    "            num_operators.append([output_dim, 0])\n",
    "            operators.append([['identity'], [None]])\n",
    "            \n",
    "        y_masked = sympy.zeros(1, num_operators[i][0] + num_operators[i][1])\n",
    "        \n",
    "        unary_mask = sympy.Matrix(tf.where(unary - unary_t > 0., 1., 0.))\n",
    "        binary_mask = sympy.Matrix(tf.where(binary - binary_t > 0., 1., 0.))\n",
    "        for j in range(num_operators[i][0]):\n",
    "            idx = np.mod(j, len(operators[i][0]))\n",
    "            if i < num_hidden_layers:\n",
    "                y_masked[0,j] = (unary_mask[j] * math_operation('sympy',operators[i][0][idx],x_masked[0,j]) +\n",
    "                             (1.0 - unary_mask[j]) * math_operation('sympy','identity',x_masked[0,j]))\n",
    "            elif i == num_hidden_layers:\n",
    "                y_masked[0,j] = x_masked[0,j]\n",
    "        for j in range(num_operators[i][1]):\n",
    "            idx = np.mod(j, len(operators[i][1]))\n",
    "            y_masked[0,num_operators[i][0]+j] = (binary_mask[j] * math_operation('sympy',operators[i][1][idx],x_masked[0,num_operators[i][0]+2*j],x_masked[0,num_operators[i][0]+2*j+1]) +\n",
    "                                              (1.0 - binary_mask[j]) * math_operation('sympy','+',x_masked[0,num_operators[i][0]+2*j],x_masked[0,num_operators[i][0]+2*j+1]))\n",
    "        x_masked = y_masked.evalf(significant_digits)\n",
    "    \n",
    "    sparsity_model = num_masks/num_weights\n",
    "    sparsity_unary = num_unary_masks/num_unary\n",
    "    sparsity_binary = num_binary_masks/num_binary\n",
    "    \n",
    "    complexity = []\n",
    "    for j in range(len(x_masked)):\n",
    "        c = 0\n",
    "        for tree_node in sympy.preorder_traversal(x_masked[j]):\n",
    "            c += 1\n",
    "        complexity.append(c)\n",
    "    \n",
    "    return x_masked, complexity, sparsity_input, sparsity_model, sparsity_unary, sparsity_binary\n",
    "  \n",
    "expressions_masked, complexity, sparsity_input, sparsity_model, sparsity_unary, sparsity_binary = get_expressions(nsr)\n",
    "print('Unroll network into symbolic expressions (input sparsity = {0:.3f}; model sparsity = {1:.3f}; unary sparsity = {2:.3f}; binary sparsity = {2:.3f})\\n--------------'.format(sparsity_input, sparsity_model, sparsity_unary, sparsity_binary))\n",
    "print('Mean complexity = {0:.1f}\\n--------------'.format(np.mean(complexity)))\n",
    "for i in range(expressions_masked.shape[1]):\n",
    "    print('expr_{0} (complexity = {1}):\\n\\n{2}\\n-------------------------------------'.format(i,complexity[i],expressions_masked[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d00c6-1966-4f38-bb31-d5f7eceda98f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f5524c-b86c-42ee-b858-e320ef1c6b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smart-pixel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
